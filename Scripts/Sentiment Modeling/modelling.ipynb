{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #natural language tool kit for tokenization, stop words, lemmatization\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import tree, ensemble, model_selection, metrics\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (17922, 24771), test: (4481, 24771)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "current_path = os.getcwd()\n",
    "pickle_in = open('data.pickle', 'rb')\n",
    "train_data = pickle.load(pickle_in)\n",
    "\n",
    "#lowercase\n",
    "train_data['text'] = train_data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "#punctuations removal\n",
    "def punctuation_removal(text):\n",
    "    return text.translate(str.maketrans('','',string.punctuation))\n",
    "train_data['text'] = train_data['text'].apply(punctuation_removal)\n",
    "\n",
    "#stopwords removal\n",
    "stopwords_lst = stopwords.words('english')\n",
    "def stopwords_removal(text):\n",
    "    text_lst = text.split()\n",
    "    text_lst = [word for word in text_lst if word.lower() not in stopwords_lst]\n",
    "    text = ' '.join(text_lst)\n",
    "    return text\n",
    "train_data['text'] = train_data['text'].apply(stopwords_removal)\n",
    "\n",
    "#lemmatization\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmawords = [lemmatizer.lemmatize(w) for w in text.split()]\n",
    "    text = ' '.join(lemmawords)\n",
    "    return text\n",
    "train_data['text'] = train_data['text'].apply(lemmatize)\n",
    "\n",
    "\n",
    "#train-test split\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "train, test = train_test_split(train_data, test_size = 0.2, random_state=2022)\n",
    "\n",
    "#vectorize text data \n",
    "#NOTE: simple way to both tokenize a collection of text documents and build a vocabulary of known words\n",
    "vect = CountVectorizer()\n",
    "train_X_dtm = vect.fit_transform(train['text'])\n",
    "test_X_dtm = vect.transform(test['text'])\n",
    "\n",
    "#prepare train data and its labels\n",
    "x_train = train_X_dtm\n",
    "y_train = train['target']\n",
    "\n",
    "#prepare test data and its labels\n",
    "x_test = test_X_dtm\n",
    "y_test = test['target']\n",
    "\n",
    "print(f'train: {train_X_dtm.shape}, test: {test_X_dtm.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to print sentiments of a sentence\n",
    "def print_sentiment_score(sentence):\n",
    "    SI_object = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    #get dictionary of sentiments of sentence\n",
    "    sentiment_dict = SI_object.polarity_scores(sentence)\n",
    "\n",
    "    #get individual scores\n",
    "    neg = sentiment_dict['neg']*100\n",
    "    neu = sentiment_dict['neu']*100\n",
    "    pos = sentiment_dict['pos']*100\n",
    "    overall = sentiment_dict['compound']\n",
    "    \n",
    "    #overall sentiment result of sentence\n",
    "    if overall>=0.05:\n",
    "        result = 'Positive'  \n",
    "    elif overall<= -0.05:\n",
    "        result = 'Negative'\n",
    "    else:\n",
    "        result = 'Neutral'\n",
    "        \n",
    "    print(f\"Negative: {neg}\\nNeutral: {neu}\\nPositive: {pos}\\nOverall: {overall}\\nResult: {result}\")\n",
    "\n",
    "#function to return overall sentiment score of a sentence\n",
    "def get_sentiment_score(sentence):\n",
    "    SI_object = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    #get dictionary of sentiments of sentence\n",
    "    sentiment_dict = SI_object.polarity_scores(sentence)\n",
    "\n",
    "    #get overall sentiment score\n",
    "    overall = sentiment_dict['compound']\n",
    "    \n",
    "    return overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4019"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Damn for a second I thought this was about the cabbages guy from ATLA. Anyway, buying GME right now...'\n",
    "get_sentiment_score(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7426913635349253"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model\n",
    "rforest = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "rforest.fit(x_train, y_train)\n",
    "\n",
    "#predict\n",
    "rf_y_pred_class = rforest.predict(x_test)\n",
    "\n",
    "#accuracy\n",
    "rf_accuracy = metrics.accuracy_score(y_test, rf_y_pred_class)\n",
    "rf_accuracy #0.7426913635349253"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7176969426467307"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#fit training data\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train)\n",
    "\n",
    "#predict\n",
    "nb_y_pred_class = nb.predict(x_test)\n",
    "\n",
    "#accuracy\n",
    "nb_accuracy = metrics.accuracy_score(y_test, nb_y_pred_class)\n",
    "nb_accuracy #0.7176969426467307"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7228297255076992"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit training data\n",
    "bagging = ensemble.BaggingClassifier(tree.DecisionTreeClassifier(), n_estimators=100, max_samples=x_train.shape[0], max_features=x_train.shape[1], random_state=2022)\n",
    "bagging.fit(x_train, y_train)\n",
    "\n",
    "#predict\n",
    "bagging_y_pred_class = bagging.predict(x_test)\n",
    "\n",
    "#accuracy\n",
    "bagging_accuracy = metrics.accuracy_score(y_test, bagging_y_pred_class)\n",
    "bagging_accuracy #0.7228297255076992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "561/561 [==============================] - 146s 254ms/step - loss: 0.8192 - accuracy: 0.5976 - val_loss: 0.5963 - val_accuracy: 0.7309\n",
      "Epoch 2/15\n",
      "561/561 [==============================] - 141s 251ms/step - loss: 0.5382 - accuracy: 0.7862 - val_loss: 0.5725 - val_accuracy: 0.7431\n",
      "Epoch 3/15\n",
      "561/561 [==============================] - 135s 240ms/step - loss: 0.4092 - accuracy: 0.8458 - val_loss: 0.6254 - val_accuracy: 0.7440\n",
      "Epoch 4/15\n",
      "561/561 [==============================] - 136s 242ms/step - loss: 0.3240 - accuracy: 0.8838 - val_loss: 0.6529 - val_accuracy: 0.7443\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 5/15\n",
      "561/561 [==============================] - 136s 242ms/step - loss: 0.2359 - accuracy: 0.9174 - val_loss: 0.7609 - val_accuracy: 0.7447\n",
      "Epoch 6/15\n",
      "561/561 [==============================] - 143s 254ms/step - loss: 0.2166 - accuracy: 0.9233 - val_loss: 0.7906 - val_accuracy: 0.7454\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 7/15\n",
      "561/561 [==============================] - 134s 239ms/step - loss: 0.2108 - accuracy: 0.9258 - val_loss: 0.7895 - val_accuracy: 0.7451\n",
      "Epoch 8/15\n",
      "561/561 [==============================] - 138s 245ms/step - loss: 0.2019 - accuracy: 0.9301 - val_loss: 0.7979 - val_accuracy: 0.7460\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 9/15\n",
      "561/561 [==============================] - 141s 251ms/step - loss: 0.1985 - accuracy: 0.9298 - val_loss: 0.7982 - val_accuracy: 0.7458\n",
      "Epoch 10/15\n",
      "561/561 [==============================] - 136s 243ms/step - loss: 0.2026 - accuracy: 0.9278 - val_loss: 0.7980 - val_accuracy: 0.7463\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 11/15\n",
      "561/561 [==============================] - 140s 249ms/step - loss: 0.2025 - accuracy: 0.9256 - val_loss: 0.7980 - val_accuracy: 0.7463\n",
      "Epoch 12/15\n",
      "561/561 [==============================] - 137s 244ms/step - loss: 0.1997 - accuracy: 0.9295 - val_loss: 0.7980 - val_accuracy: 0.7463\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 13/15\n",
      "561/561 [==============================] - 140s 249ms/step - loss: 0.2024 - accuracy: 0.9257 - val_loss: 0.7980 - val_accuracy: 0.7463\n",
      "Epoch 14/15\n",
      "561/561 [==============================] - 138s 245ms/step - loss: 0.1971 - accuracy: 0.9290 - val_loss: 0.7980 - val_accuracy: 0.7463\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 15/15\n",
      "561/561 [==============================] - 152s 271ms/step - loss: 0.1983 - accuracy: 0.9286 - val_loss: 0.7980 - val_accuracy: 0.7463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb67b39f820>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, Embedding, GlobalAveragePooling1D, SpatialDropout1D, Bidirectional, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# preprocess data\n",
    "train_sentences = list(train['text'])\n",
    "train_labels = list(train['target'])\n",
    "\n",
    "training_size = round(len(train_sentences)*1)\n",
    "\n",
    "training_sentences = train_sentences[0:training_size]\n",
    "training_labels = train_labels[0:training_size]\n",
    "\n",
    "valid_sentences = train_sentences[training_size:]\n",
    "valid_labels = train_labels[training_size:]\n",
    "\n",
    "test_sentences = list(test['text'])\n",
    "test_labels = list(test['target'])\n",
    "\n",
    "# Setting tokenizer properties\n",
    "vocab_size = 24500\n",
    "oov_tok = \"<oov>\"\n",
    "\n",
    "# Fit the tokenizer on Training data\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "\n",
    "# Setting the padding properties\n",
    "sequence_length = 300 #longest text is 271 in dataset\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "\n",
    "# Creating padded sequences from train and test data\n",
    "training_sentences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sentences, maxlen=sequence_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "valid_sentences = tokenizer.texts_to_sequences(valid_sentences)\n",
    "valid_padded = pad_sequences(valid_sentences, maxlen=sequence_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=sequence_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Setting the model parameters\n",
    "embedding_dim = 40\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience = 5, restore_best_weights=True)\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(\n",
    "                                        monitor='val_loss', \n",
    "                                        factor=0.1, \n",
    "                                        patience=2, \n",
    "                                        verbose=1,\n",
    "                                        mode='min', \n",
    "                                        min_delta=0.0001, \n",
    "                                        cooldown=0, \n",
    "                                        min_lr=0,\n",
    "                                        )\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, embedding_dim, input_length =sequence_length),\n",
    "  SpatialDropout1D(0.5),\n",
    "  # GlobalAveragePooling1D(),\n",
    "  # Conv1D(128, 1),\n",
    "  Bidirectional(LSTM(units=128), name='bd_1'),\n",
    "  # LSTM(units=128),\n",
    "  # Bidirectional(LSTM(units=64, return_sequences=True), name='bd_1'),\n",
    "  # Dropout(0.6),\n",
    "  # Bidirectional(LSTM(units=32, return_sequences=False), name='bd_2'),\n",
    "  # LSTM(64, dropout = 0.2, recurrent_dropout=0.2),\n",
    "  # Dense(128, activation='relu'),\n",
    "  Dropout(0.5),\n",
    "  Dense(32, activation='relu'),\n",
    "  Dropout(0.5),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(3, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# adam = Adam(learning_rate = 1e-5)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#convert train data to numpy array\n",
    "x_train_padded = np.array(training_padded)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "x_valid_padded = np.array(valid_padded)\n",
    "valid_labels =np.array(valid_labels)\n",
    "\n",
    "x_test_padded = np.array(test_padded)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "training_labels = to_categorical(training_labels, 3)\n",
    "valid_labels = to_categorical(valid_labels, 3)\n",
    "test_labels = to_categorical(test_labels, 3)\n",
    "\n",
    "#train model\n",
    "num_epochs = 15\n",
    "model.fit(x_train_padded, training_labels, epochs=num_epochs, validation_data=(x_test_padded, test_labels), callbacks=[reduce_lr_on_plateau])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 9s 67ms/step - loss: 0.7980 - accuracy: 0.7463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7980167865753174, 0.7462620139122009]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.evaluate(x_test_padded, test_labels)\n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_44 (Embedding)     (None, 300, 40)           980000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_40 (Spatia (None, 300, 40)           0         \n",
      "_________________________________________________________________\n",
      "bd_1 (Bidirectional)         (None, 256)               173056    \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 1,161,379\n",
      "Trainable params: 1,161,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "561/561 [==============================] - 7s 10ms/step - loss: 1.0238 - accuracy: 0.4376 - val_loss: 0.9899 - val_accuracy: 0.4512\n",
      "Epoch 2/50\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 0.9482 - accuracy: 0.4973 - val_loss: 0.8968 - val_accuracy: 0.6010\n",
      "Epoch 3/50\n",
      "561/561 [==============================] - 5s 9ms/step - loss: 0.8657 - accuracy: 0.6002 - val_loss: 0.8354 - val_accuracy: 0.6233\n",
      "Epoch 4/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.7861 - accuracy: 0.6584 - val_loss: 0.7767 - val_accuracy: 0.6606\n",
      "Epoch 5/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.7198 - accuracy: 0.6897 - val_loss: 0.7346 - val_accuracy: 0.6762\n",
      "Epoch 6/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.6591 - accuracy: 0.7189 - val_loss: 0.6992 - val_accuracy: 0.6943\n",
      "Epoch 7/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.6022 - accuracy: 0.7521 - val_loss: 0.6734 - val_accuracy: 0.7195\n",
      "Epoch 8/50\n",
      "561/561 [==============================] - 5s 9ms/step - loss: 0.5518 - accuracy: 0.7752 - val_loss: 0.6498 - val_accuracy: 0.7242\n",
      "Epoch 9/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.5182 - accuracy: 0.7898 - val_loss: 0.6384 - val_accuracy: 0.7224\n",
      "Epoch 10/50\n",
      "561/561 [==============================] - 5s 9ms/step - loss: 0.4848 - accuracy: 0.8053 - val_loss: 0.6369 - val_accuracy: 0.7244\n",
      "Epoch 11/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.4500 - accuracy: 0.8233 - val_loss: 0.6378 - val_accuracy: 0.7262\n",
      "Epoch 12/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.4243 - accuracy: 0.8363 - val_loss: 0.6405 - val_accuracy: 0.7277\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.4045 - accuracy: 0.8419 - val_loss: 0.6412 - val_accuracy: 0.7289\n",
      "Epoch 14/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.4051 - accuracy: 0.8432 - val_loss: 0.6418 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 15/50\n",
      "561/561 [==============================] - 5s 9ms/step - loss: 0.3980 - accuracy: 0.8462 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 16/50\n",
      "561/561 [==============================] - 7s 13ms/step - loss: 0.4015 - accuracy: 0.8457 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 17/50\n",
      "561/561 [==============================] - 6s 12ms/step - loss: 0.3911 - accuracy: 0.8480 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 18/50\n",
      "561/561 [==============================] - 5s 9ms/step - loss: 0.3999 - accuracy: 0.8456 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 19/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.3994 - accuracy: 0.8464 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 20/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.4021 - accuracy: 0.8470 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 21/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.3977 - accuracy: 0.8456 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 22/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.3959 - accuracy: 0.8467 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 23/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.4010 - accuracy: 0.8451 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 24/50\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 0.3963 - accuracy: 0.8480 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 25/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.3989 - accuracy: 0.8470 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 26/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.3973 - accuracy: 0.8447 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "Epoch 27/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.4019 - accuracy: 0.8459 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 28/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.3983 - accuracy: 0.8486 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "Epoch 29/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.3977 - accuracy: 0.8481 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 30/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.4013 - accuracy: 0.8420 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "Epoch 31/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.3965 - accuracy: 0.8489 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 32/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.3995 - accuracy: 0.8450 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "Epoch 33/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.3961 - accuracy: 0.8481 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 34/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.3998 - accuracy: 0.8454 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "Epoch 35/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.3974 - accuracy: 0.8446 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 36/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.3982 - accuracy: 0.8443 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "Epoch 37/50\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 0.3953 - accuracy: 0.8475 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 38/50\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 0.4044 - accuracy: 0.8399 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "Epoch 39/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.3949 - accuracy: 0.8467 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 40/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.3978 - accuracy: 0.8466 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "Epoch 41/50\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 0.3996 - accuracy: 0.8500 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 42/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.3990 - accuracy: 0.8446 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "Epoch 43/50\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 0.4001 - accuracy: 0.8452 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 44/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.4000 - accuracy: 0.8476 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
      "Epoch 45/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.4031 - accuracy: 0.8430 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 46/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.4028 - accuracy: 0.8424 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
      "Epoch 47/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.4043 - accuracy: 0.8453 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 48/50\n",
      "561/561 [==============================] - 4s 7ms/step - loss: 0.4005 - accuracy: 0.8471 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
      "Epoch 49/50\n",
      "561/561 [==============================] - 5s 8ms/step - loss: 0.3946 - accuracy: 0.8475 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "Epoch 50/50\n",
      "561/561 [==============================] - 4s 8ms/step - loss: 0.4050 - accuracy: 0.8420 - val_loss: 0.6419 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000944832675e-23.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb6a8924b20>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length =sequence_length),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation = 'softmax')])\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "num_epochs = 50\n",
    "model2.fit(x_train_padded, training_labels, epochs=num_epochs, validation_data=(x_test_padded, test_labels), callbacks=[reduce_lr_on_plateau])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 0s 2ms/step - loss: 0.6419 - accuracy: 0.7291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6419309973716736, 0.7290783524513245]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(x_test_padded, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_48 (Embedding)     (None, 300, 40)           980000    \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 300, 40)           0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 300, 32)           1312      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_9 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 981,411\n",
      "Trainable params: 981,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.keras.utils.to_categorical(train_data['target'], 3)[4] \n",
    "#-1: [0,0,1], 0: [1,0,0], 1: [0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -1\n",
       "1       -1\n",
       "2       -1\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "22398    1\n",
       "22399    0\n",
       "22400    1\n",
       "22401   -1\n",
       "22402    0\n",
       "Name: target, Length: 22403, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f4714730b1b751e7b7741385746c606ffaf14712c97a43308c3088f3a6e405a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
